🧠 ローカルLLMチャットボット開発計画

-------------------------------------
🎯 目的
-------------------------------------
ローカルで動作する大規模言語モデル（LLM）を活用し、
インターネット接続なしで動作するチャットボットアプリを構築する。

-------------------------------------
🧩 技術スタック
-------------------------------------
| 役割 | 技術 | 理由 |
|------|------|------|
| フロントエンド | React + TypeScript + Vite | 軽量・モダン・開発しやすい |
| バックエンド | FastAPI (Python) | 非同期・高速・実装が簡単 |
| LLM実行環境 | Ollama (例: Llama 3, Mistral, Gemma) | MacのGPU（Metal）最適化済み |
| 通信方式 | REST API (HTTP POST) | シンプルに実装可能 |

-------------------------------------
💻 システム構成図
-------------------------------------
┌───────────────┐
│ React (Vite) │ ←→ axios/fetch → FastAPI
└───────────────┘
         ↓
┌───────────────────┐
│ FastAPI (Python)  │
│ /chat → ollama run│
└───────────────────┘
         ↓
┌───────────────────┐
│ Ollama LLM Engine │
│ llama3, mistral...│
└───────────────────┘

-------------------------------------
🗓️ 開発ステップ
-------------------------------------

STEP 1. 環境構築（Mac向け）
-------------------------------------
brew install ollama
ollama serve
ollama pull llama3
ollama run llama3  # テスト

STEP 2. Python バックエンド構築
-------------------------------------
mkdir backend && cd backend
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn

main.py
-------------------------------------
from fastapi import FastAPI
from pydantic import BaseModel
import subprocess
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    prompt: str

@app.post("/chat")
def chat(req: ChatRequest):
    result = subprocess.run(
        ["ollama", "run", "llama3", req.prompt],
        capture_output=True, text=True
    )
    return {"response": result.stdout.strip()}

# 実行
uvicorn main:app --reload


STEP 3. React フロントエンド構築
-------------------------------------
cd ..
npm create vite@latest frontend -- --template react-ts
cd frontend
npm install
npm run dev

src/App.tsx
-------------------------------------
import { useState } from "react";

function App() {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState<string[]>([]);

  const sendMessage = async () => {
    const res = await fetch("http://localhost:8000/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ prompt: input }),
    });
    const data = await res.json();
    setMessages((m) => [...m, "👤 " + input, "🤖 " + data.response]);
    setInput("");
  };

  return (
    <div className="p-6 max-w-xl mx-auto">
      <h1 className="text-xl font-bold mb-4">Local LLM Chatbot</h1>
      <div className="border rounded p-2 h-80 overflow-y-auto bg-gray-50">
        {messages.map((m, i) => <div key={i}>{m}</div>)}
      </div>
      <div className="mt-4 flex">
        <input
          className="border flex-1 p-2"
          value={input}
          onChange={(e) => setInput(e.target.value)}
        />
        <button
          className="ml-2 bg-blue-500 text-white px-4"
          onClick={sendMessage}
        >
          Send
        </button>
      </div>
    </div>
  );
}

export default App;

-------------------------------------
🚀 動作確認
-------------------------------------
1. バックエンド起動
   uvicorn main:app --reload

2. フロントエンド起動
   npm run dev

3. ブラウザでアクセス
   → http://localhost:5173
   → 入力したテキストに LLM が応答すれば成功！

-------------------------------------
🧠 拡張アイデア
-------------------------------------
| 機能 | 概要 |
|------|------|
| ストリーミング応答 | ollama run --stream を利用してリアルタイム応答 |
| 会話履歴保存 | Python 側で履歴管理 / SQLite 使用 |
| モデル切り替え | APIで model パラメータを指定（例: llama3, mistral） |
| UI強化 | TailwindCSS / shadcn/ui を導入 |
| デスクトップ化 | Electron + React + FastAPI でアプリ化 |

-------------------------------------
🗓️ スケジュール（目安）
-------------------------------------
| 期間 | 内容 |
|------|------|
| 1日目 | 環境構築（Python, Node, Ollama） |
| 2日目 | FastAPIでLLM呼び出しAPI作成 |
| 3日目 | ReactでチャットUI作成 |
| 4〜5日目 | 会話履歴・UI改善 |
| 1週間後 | ストリーミング・モデル拡張 |

-------------------------------------
⚙️ 今後の選択肢
-------------------------------------
次の開発ステップとして、以下のいずれかを選択可能：

1. 🌀 ストリーミング応答対応（リアルタイム出力）
2. 🧩 複数モデル切り替え機能（例: Llama3 / Mistral）
3. 💾 会話履歴管理機能（セッション対応）

-------------------------------------
開発環境
-------------------------------------
MacBook Pro (メモリ24GB, GPUあり)
→ Ollama + FastAPI + React 構成が最適
