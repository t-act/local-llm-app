ğŸ§  ãƒ­ãƒ¼ã‚«ãƒ«LLMãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆé–‹ç™ºè¨ˆç”»

-------------------------------------
ğŸ¯ ç›®çš„
-------------------------------------
ãƒ­ãƒ¼ã‚«ãƒ«ã§å‹•ä½œã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ã€
ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãªã—ã§å‹•ä½œã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚¢ãƒ—ãƒªã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

-------------------------------------
ğŸ§© æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
-------------------------------------
| å½¹å‰² | æŠ€è¡“ | ç†ç”± |
|------|------|------|
| ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ | React + TypeScript + Vite | è»½é‡ãƒ»ãƒ¢ãƒ€ãƒ³ãƒ»é–‹ç™ºã—ã‚„ã™ã„ |
| ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ | FastAPI (Python) | éåŒæœŸãƒ»é«˜é€Ÿãƒ»å®Ÿè£…ãŒç°¡å˜ |
| LLMå®Ÿè¡Œç’°å¢ƒ | Ollama (ä¾‹: Llama 3, Mistral, Gemma) | Macã®GPUï¼ˆMetalï¼‰æœ€é©åŒ–æ¸ˆã¿ |
| é€šä¿¡æ–¹å¼ | REST API (HTTP POST) | ã‚·ãƒ³ãƒ—ãƒ«ã«å®Ÿè£…å¯èƒ½ |

-------------------------------------
ğŸ’» ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³
-------------------------------------
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ React (Vite) â”‚ â†â†’ axios/fetch â†’ FastAPI
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FastAPI (Python)  â”‚
â”‚ /chat â†’ ollama runâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ollama LLM Engine â”‚
â”‚ llama3, mistral...â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

-------------------------------------
ğŸ—“ï¸ é–‹ç™ºã‚¹ãƒ†ãƒƒãƒ—
-------------------------------------

STEP 1. ç’°å¢ƒæ§‹ç¯‰ï¼ˆMacå‘ã‘ï¼‰
-------------------------------------
brew install ollama
ollama serve
ollama pull llama3
ollama run llama3  # ãƒ†ã‚¹ãƒˆ

STEP 2. Python ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰æ§‹ç¯‰
-------------------------------------
mkdir backend && cd backend
python3 -m venv venv
source venv/bin/activate
pip install fastapi uvicorn

main.py
-------------------------------------
from fastapi import FastAPI
from pydantic import BaseModel
import subprocess
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    prompt: str

@app.post("/chat")
def chat(req: ChatRequest):
    result = subprocess.run(
        ["ollama", "run", "llama3", req.prompt],
        capture_output=True, text=True
    )
    return {"response": result.stdout.strip()}

# å®Ÿè¡Œ
uvicorn main:app --reload


STEP 3. React ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰æ§‹ç¯‰
-------------------------------------
cd ..
npm create vite@latest frontend -- --template react-ts
cd frontend
npm install
npm run dev

src/App.tsx
-------------------------------------
import { useState } from "react";

function App() {
  const [input, setInput] = useState("");
  const [messages, setMessages] = useState<string[]>([]);

  const sendMessage = async () => {
    const res = await fetch("http://localhost:8000/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ prompt: input }),
    });
    const data = await res.json();
    setMessages((m) => [...m, "ğŸ‘¤ " + input, "ğŸ¤– " + data.response]);
    setInput("");
  };

  return (
    <div className="p-6 max-w-xl mx-auto">
      <h1 className="text-xl font-bold mb-4">Local LLM Chatbot</h1>
      <div className="border rounded p-2 h-80 overflow-y-auto bg-gray-50">
        {messages.map((m, i) => <div key={i}>{m}</div>)}
      </div>
      <div className="mt-4 flex">
        <input
          className="border flex-1 p-2"
          value={input}
          onChange={(e) => setInput(e.target.value)}
        />
        <button
          className="ml-2 bg-blue-500 text-white px-4"
          onClick={sendMessage}
        >
          Send
        </button>
      </div>
    </div>
  );
}

export default App;

-------------------------------------
ğŸš€ å‹•ä½œç¢ºèª
-------------------------------------
1. ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰èµ·å‹•
   uvicorn main:app --reload

2. ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰èµ·å‹•
   npm run dev

3. ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹
   â†’ http://localhost:5173
   â†’ å…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã« LLM ãŒå¿œç­”ã™ã‚Œã°æˆåŠŸï¼

-------------------------------------
ğŸ§  æ‹¡å¼µã‚¢ã‚¤ãƒ‡ã‚¢
-------------------------------------
| æ©Ÿèƒ½ | æ¦‚è¦ |
|------|------|
| ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­” | ollama run --stream ã‚’åˆ©ç”¨ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­” |
| ä¼šè©±å±¥æ­´ä¿å­˜ | Python å´ã§å±¥æ­´ç®¡ç† / SQLite ä½¿ç”¨ |
| ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆ | APIã§ model ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒ‡å®šï¼ˆä¾‹: llama3, mistralï¼‰ |
| UIå¼·åŒ– | TailwindCSS / shadcn/ui ã‚’å°å…¥ |
| ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—åŒ– | Electron + React + FastAPI ã§ã‚¢ãƒ—ãƒªåŒ– |

-------------------------------------
ğŸ—“ï¸ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆç›®å®‰ï¼‰
-------------------------------------
| æœŸé–“ | å†…å®¹ |
|------|------|
| 1æ—¥ç›® | ç’°å¢ƒæ§‹ç¯‰ï¼ˆPython, Node, Ollamaï¼‰ |
| 2æ—¥ç›® | FastAPIã§LLMå‘¼ã³å‡ºã—APIä½œæˆ |
| 3æ—¥ç›® | Reactã§ãƒãƒ£ãƒƒãƒˆUIä½œæˆ |
| 4ã€œ5æ—¥ç›® | ä¼šè©±å±¥æ­´ãƒ»UIæ”¹å–„ |
| 1é€±é–“å¾Œ | ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ»ãƒ¢ãƒ‡ãƒ«æ‹¡å¼µ |

-------------------------------------
âš™ï¸ ä»Šå¾Œã®é¸æŠè‚¢
-------------------------------------
æ¬¡ã®é–‹ç™ºã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’é¸æŠå¯èƒ½ï¼š

1. ğŸŒ€ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”å¯¾å¿œï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›ï¼‰
2. ğŸ§© è¤‡æ•°ãƒ¢ãƒ‡ãƒ«åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½ï¼ˆä¾‹: Llama3 / Mistralï¼‰
3. ğŸ’¾ ä¼šè©±å±¥æ­´ç®¡ç†æ©Ÿèƒ½ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³å¯¾å¿œï¼‰

-------------------------------------
é–‹ç™ºç’°å¢ƒ
-------------------------------------
MacBook Pro (ãƒ¡ãƒ¢ãƒª24GB, GPUã‚ã‚Š)
â†’ Ollama + FastAPI + React æ§‹æˆãŒæœ€é©
